# enetLTS: Robust and Sparse Methods for High Dimensional Linear and Binary and Multinomial Regression

## Summary

`enetLTS` is an `R` package that provides a fully robust version of 
elastic net estimator for high dimensional linear and binary and multinomial regression. 
The elastic net penalization provides 
intrinsic variable selection and coefficient estimates for highly correlated 
variables in particular for high-dimensional low sample size 
data sets, and it has been extended to generalized linear regression models 
([Friedman et al., 2010](https://www.jstatsoft.org/article/download/v033i01/361)). 
Combining these advantages with trimming idea yields the robust solutions.
The main idea of the algorithm is to search for outlier-free subsets on which the classical elastic 
net estimator can be applied. Outlier-free subsets are determined by trimming 
the penalized log-likelihood function belonging to the regression model. 
The algorithm starts with 500 elemental subsets
only for one combination of $\alpha$ and $\lambda$, and takes the *warm start* strategy
for subsequent combinations in order to save the computation time.
The final reweighting step is added to improve the statistical 
efficiency of the proposed methods. 
From this point of view, the enet-LTS estimator can be seen as trimmed version 
of the elastic net regression estimator for linear, binary and multinomial 
regression ([Friedman et al., 2010](https://www.jstatsoft.org/article/download/v033i01/361)). 
Selecting optimal model with optimal tuning parameters is done via cross-validation, 
and various plots are available to illustrate model and to evaluate the 
final model estimates. 

## Implemented Methods 

- `enetLTS()`: elastic net trimmed squared regression for families:

   1- `gaussian`

   2- `binomial`
   
   3- `multinomial`
                                                                  

## Installation

Package `enetLTS` is on CRAN (The Comprehensive `R` Archive Network), hence the latest release can be easily installed from the `R` command as follows

```R
> install.packages("enetLTS")
```

## Building from source

To install the latest (possibly unstable) version from GitHub, you can pull this repository and install it from the `R` command line as follows

```R
> install.packages("devtools")
> devtools::install_github("fatmasevinck/enetLTS")
```

If you already have package `devtools` installed, the first line can be skipped.


# Example: Robust and Sparse Linear Regression

We have considered the [NCI-60 cancer cell panel](https://discover.nci.nih.gov/cellminer/) data in order to illustrate the functionality of the `enetLTS` model for linear regression. As in ([Alfons, 2021](https://joss.theoj.org/papers/10.21105/joss.03786)), the response variable is determined by the protein expressions for a specific protein, which is 92th protein, and
the explanatory variable is determined by the gene expressions of the 100 genes that have the highest (robustly estimated) correlations with the response variable. This data set is available in package `robustHD`.

```R
> # load data
> library("robustHD")
> data("nci60")  # contains matrices 'protein' and 'gene'

> # define response variable
> y <- protein[, 92]
> # screen most correlated predictor variables
> correlations <- apply(gene, 2, corHuber, y)
> keep <- partialOrder(abs(correlations), 100, decreasing = TRUE)
> X <- gene[, keep]
```

Like many other packages, the easy way to use the package `enetLTS` is to install it directly from `CRAN`. 

```R
> # install and load package
> install.packages("enetLTS")
> library(enetLTS)
> # fit the model for family="gaussian"
> fit.gaussian <- enetLTS(X,y)
> [1] "optimal model: lambda = 0.1391 alpha = 0.6"
>
> fit.gaussian
enetLTS estimator 

Call:  enetLTS(xx = X, yy = y, family = "gaussian", alphas = alphas, lambdas = lambdas, lambdaw = NULL, intercept = TRUE, scal = TRUE, 
 hsize = 0.75, nsamp = 500, nCsteps = 20, nfold = 5, repl = 1, ncores = 1, tol = -1e+06, seed = NULL, crit.plot = TRUE) 


Coefficients:
           1            2            3            4            5            6            7            8            9           10 
-5.227875054  0.240931448  0.000000000  0.116076316  0.027573388  0.000000000  0.000000000  0.000000000  0.000000000  0.041368849 
          11           12           13           14           15           16           17           18           19           20 
 0.000000000  0.000000000  0.032874491  0.000000000  0.000000000  0.000000000  0.000000000  0.391369317  0.053524802  0.000000000 
          21           22           23           24           25           26           27           28           29           30 
 0.000000000  0.000000000  0.000000000  0.028517873 -0.257094024  0.000000000  0.000000000  0.000000000 -0.095686659  0.000000000 
          31           32           33           34           35           36           37           38           39           40 
 0.000000000  0.000000000  0.093010871  0.000000000  0.000000000  0.000000000  0.000000000  0.000000000  0.055097698 -0.158542779 
          41           42           43           44           45           46           47           48           49           50 
 0.000000000  0.000000000  0.000000000  0.000000000  0.000000000  0.000000000 -0.042666773  0.000000000  0.000000000  0.000000000 
          51           52           53           54           55           56           57           58           59           60 
 0.000000000  0.000000000  0.000000000  0.000000000  0.000000000  0.000000000  0.000000000  0.000000000  0.000000000  0.000000000 
          61           62           63           64           65           66           67           68           69           70 
 0.000000000  0.000000000  0.000000000 -0.013522905  0.000000000  0.000000000  0.000000000  0.000000000  0.129058794  0.000000000 
          71           72           73           74           75           76           77           78           79           80 
 0.088705925  0.000000000  0.097641709  0.082569621  0.000000000  0.000000000  0.111312062  0.000000000  0.000000000  0.000000000 
          81           82           83           84           85           86           87           88           89           90 
 0.000000000  0.000000000  0.000000000  0.000000000  0.119835636 -0.046678268  0.000000000 -0.049993645  0.000000000  0.000000000 
          91           92           93           94           95           96           97           98           99          100 
 0.005319332  0.183509787  0.000000000  0.000000000  0.000000000 -0.002034250  0.000000000  0.000000000  0.040520680  0.000000000 
         101 
 0.030654977 

 number of the nonzero coefficients:
[1] 29

 alpha: 0.6
 lambda: 0.1391
 lambdaw: 0.07545663
```

`enetLTS()` 

The combination of the optimal tuning parameters is defined by 5-fold cross-validation based on 
certain grids for $\alpha$ and $\lambda$. 
Evaluation criterion for 5-fold cross-validation is summarized by heatmap for users if the arguman 
is chosen as `crit.plot="TRUE"`.

![Heatmap for 5-fold cross-validation](paper/JOSSgausHeatMap.png)


Several plots are available for the results: `plotCoef.enetLTS()` visualizes the coefficients, 
`plotResid.enetLTS()` plots the values of residuals vs fitted values, 
and `plotDiagnostic.enetLTS()` allows to produce various diagnostic
plots for the final model fit. 

![residuals (left); diagnostic (right)\label{fig:plotexamples}{width=%110}](paper/JOSSgausNCI60.png)

Examples of the residuals plot (left) and the diagnostic plot (right) for output of function 
`enetLTS()` with the arguman `family="gaussian"`.

# Example: Robust and Sparse Binary Regression 

# Example: Robust and Sparse Multinomial Regression

enetLTS estimator 

Call:  enetLTS(xx = xx, yy = yy, family = "multinomial", alphas = alphas,      lambdas = lambdas, lambdaw = NULL, intercept = TRUE, scal = TRUE,      hsize = 0.75, nsamp = c(500, 10), nCsteps = 20, nfold = 5,      repl = 1, ncores = 1, tol = -1e+06, seed = NULL, crit.plot = TRUE) 


Coefficients:
           11            12            13            14            15            16            17            18            19 
 2.7962253887 -0.2593751542  0.3775465377  0.7066047618 -3.0262227628 -2.5902303222 -1.8754419810 -0.4052527413 -3.4611180589 
          110           111           112           113           114           115           116           117           118 
-0.4987077487 -0.0257654868 -2.1613782188 -0.7325710773 -1.8718226874 -2.4933431563  0.7047579527 -0.6140012551 -2.1597577251 
          119           120           121           122           123           124           125           126           127 
-4.4350672167  1.7666686513  0.0000000000 -0.9321995561 -0.9365850370 -1.2891400830  0.0298118632  2.6273452659  0.7175396338 
          128           129           130           131           132           133           134           135           136 
-1.7248354438 -0.7235180459  0.2375035311  1.4344107589  0.0000000000  0.2159089204  0.5473201751  0.5806706713  0.5614062059 
          137           138           139           140           141           142           143           144           145 
 0.4121320921  0.8639475749 -0.2656923988  0.8847615828  0.4869034920  0.5340535805  0.3417365543  0.1445993443  0.2491952271 
          146           147           148           149           150           151           152           153           154 
 0.1390018582  0.2558278931  0.4168440976  0.2707618711  0.2161133700  0.1952721380  0.1683127787  0.0971066986  0.2071595210 
          155           156           157           158           159           160           161           162           163 
 0.2831204408  0.2129688780  0.2595160689  0.3227035264  0.2052215795  0.1003195253  0.1280915066  0.0316268968  0.0000000000 
          164           165           166           167           168           169           170           171           172 
 0.0502885490  0.0000000000  0.0000000000 -0.0211106914 -0.0448813983 -0.0996250701 -0.1170973198 -0.0843481621 -0.0810415941 
          173           174           175           176           177           178           179           180           181 
-0.0137636236  0.0000000000 -0.0006301308 -0.0402869600 -0.0286739779 -0.0064184976  0.0000000000  0.0000000000  0.0178923013 
          182           183           184           185           186           187           188           189           190 
 0.0778690335  0.1105323255  0.0992363060  0.0922876450  0.1017756120  0.0572848892  0.0242475345  0.0378020214  0.0130600479 
          191           192           193           194           195           196           197           198           199 
 0.0126789885  0.0422915996  0.0454754934  0.0733651455  0.1244594582  0.1441657481  0.1459085882  0.1339793815  0.1283207835 
         1100          1101          1102          1103          1104          1105          1106          1107          1108 
 0.0848832735  0.1089957730  0.0574430058  0.0071252112  0.0000000000  0.0000000000  0.0166778989  0.0349868272  0.1123420404 
         1109          1110          1111          1112          1113          1114          1115          1116          1117 
 0.1369208463  0.1898646712  0.2177059653  0.2469031490  0.2814930651  0.2525260208  0.2686475109  0.2517810325  0.1939811989 
         1118          1119          1120          1121          1122          1123          1124          1125          1126 
 0.0941982385  0.0000000000 -0.2852201020 -0.6504349634 -1.0668771400 -1.5072514641 -1.9326920853 -2.2965667559 -2.5609578945 
         1127          1128          1129          1130          1131          1132          1133          1134          1135 
-2.6688266936 -2.5822670464 -2.2873685311 -1.8531575500 -1.3412664949 -0.8429975904 -0.4261018552 -0.0878172449  0.0007142593 
         1136          1137          1138          1139          1140          1141          1142          1143          1144 
 0.2444104645  0.4164832653  0.5364663235  0.6440113776  0.7082631226  0.7830330695  0.7972849797  0.8449145374  0.8840834057 
         1145          1146          1147          1148          1149          1150          1151          1152          1153 
 0.9183550064  0.9567188235  1.0461159937  1.0402316755  1.0931905524  1.1167913510  1.1334261181  1.1242397383  1.1398335526 
         1154          1155          1156          1157          1158          1159          1160          1161          1162 
 1.1054518144  1.1001721957  1.0794401330  1.0200618658  0.9574531173  0.8638889689  0.7616531293  0.6895821901  0.6384920430 
         1163          1164          1165          1166          1167          1168          1169          1170          1171 
 0.6129767874  0.6141327267  0.5979697565  0.5982901927  0.5916799671  0.5902861871  0.5930304523  0.5948796757  0.5724308719 
         1172          1173          1174          1175          1176          1177          1178          1179          1180 
 0.5555528981  0.5287209799  0.5111383427  0.4853735035  0.4647936052  0.4467787017  0.4262081365  0.3797415965  0.3975906620 
         1181          1182          1183          1184          1185          1186          1187          1188          1189 
 0.3551337713  0.3595101676  0.3383475942  0.3075677824  0.2632439713  0.2084355983  0.1230557927  0.0622817281  0.0000000000 
         1190          1191          1192          1193          1194          1195          1196          1197          1198 
 0.0000000000 -0.1366851341 -0.3523025241 -0.5131629117 -0.6704307265 -0.7687978077 -0.8987449212 -1.0486124054 -1.2570733759 
         1199          1200          1201          1202          1203          1204          1205          1206          1207 
-1.4173438708 -1.5390184813 -1.6135247264 -1.6659504884 -1.7102082767 -1.6495559443 -1.6473118168 -1.5696933096 -1.5680102518 
         1208          1209          1210          1211          1212          1213          1214          1215          1216 
-1.5068476208 -1.4256860152 -1.3585842965 -1.2853390157 -1.1548020155 -1.0900225733 -0.9632486555 -0.7862792884 -0.6263677914 
         1217          1218          1219          1220          1221          1222          1223          1224          1225 
-0.4526594940 -0.2952121007 -0.1626010664 -0.0386683031  0.0000000000  0.0165091441  0.2195623574  0.3695822328  0.4933238010 
         1226          1227          1228          1229          1230          1231          1232          1233          1234 
 0.7412456153  0.8009040767  0.9954373058  1.0900382506  1.1949544423  1.2940680842  1.3454649130  1.1851748154  1.4364382198 
         1235          1236          1237          1238          1239          1240          1241          1242          1243 
 1.5359750652  1.5941370774  1.4355398123  2.1402812026  1.3335126519  1.2913826937  1.7398138388  1.0607187321  1.2676287782 
         1244          1245          1246          1247          1248          1249          1250          1251          1252 
 0.9134722897  1.3826692689  1.1841653391  1.1452399597  0.6438501316  0.8050253435  2.1161752006  0.2713501025  0.1574992250 
         1253          1254          1255          1256          1257          1258          1259          1260          1261 
 0.7291072345  0.2142565754  1.0204454791  1.4067931125  0.6299966065  6.7529902637 -1.4027829696 -0.8984986707 -1.3478633172 
         1262          1263          1264          1265          1266          1267          1268          1269          1270 
 0.3879219598  0.7943801558  0.1816701593 -0.3686101009  1.5055523865 -0.0535660851 -0.2159852131  0.0000000000 -0.5471757311 
         1271          1272          1273          1274          1275          1276          1277          1278          1279 
 0.1038770653  0.7919784239 -1.0372514718 -0.4295144314  0.2813565409  3.5850126819 -1.7759928846 -0.4916549702 -0.5735099659 
         1280          1281          1282          1283          1284          1285          1286          1287          1288 
-0.1363059683  1.6338275348 -1.1592781119 -2.0613851537 -1.2391260290  1.4219685563  0.2592669638  1.0075598885 -0.2480212066 
         1289          1290          1291          1292          1293          1294          1295          1296          1297 
 0.1342504290 -0.0821667624 -0.4227095760 -0.1227883876  0.0000000000  0.0000000000 -0.1440064008  0.4425342483 -0.3422788349 
         1298          1299          1300          1301          1302          1303          1304          1305          1306 
-0.0696625373 -0.1976843220 -0.0169641954  0.0000000000  0.0000000000  0.0000000000  0.0000000000 -0.0333519107  0.0000000000 
         1307          1308          1309          1310          1311          1312          1313          1314          1315 
 0.0000000000 -0.0005319644  0.0000000000  0.0080237143  0.0000000000 -0.0187021191  0.0000000000 -0.0445664644 -0.0781311709 
         1316          1317          1318          1319          1320          1321          1322          1323          1324 
 0.0000000000  0.0000000000 -0.0405212757  0.0000000000  0.0372874571  0.0000000000  0.0000000000  0.0000000000  0.0000000000 
         1325          1326          1327          1328          1329          1330          1331          1332          1333 
 0.0000000000 -0.0286913792 -0.0816638929 -0.0647812617 -0.1603999425 -0.2436933623 -0.3333091691 -0.2781610407 -0.2271086075 
         1334          1335          1336          1337          1338          1339          1340          1341          1342 
-0.1945644246 -0.2088435932 -0.2231104094 -0.2034545662 -0.2201721623 -0.1949720728 -0.2088804138 -0.2123870717 -0.1833155619 
         1343          1344          1345          1346          1347          1348          1349          1350          1351 
-0.1267464925 -0.0947090877 -0.0899690674 -0.0762105294 -0.0909129808 -0.0531668690 -0.0752573780 -0.0811894939 -0.0227849618 
         1352          1353          1354          1355          1356          1357          1358          1359          1360 
-0.0384495271 -0.0083784817 -0.0177592764  0.0000000000  0.0000000000  0.0000000000  0.0000000000  0.0029637687  0.0519673338 
         1361          1362          1363          1364          1365          1366          1367          1368          1369 
 0.0783245585  0.1427011606  0.1277915905  0.1439472629  0.1428785605  0.1385487779  0.1382475027  0.1667316899  0.1762787432 
         1370          1371          1372          1373          1374          1375          1376          1377          1378 
 0.1804090546  0.1857293732  0.1707249801  0.1199431604  0.0383414372  0.0000000000  0.0000000000  0.0122394601  0.1559035403 
         1379          1380          1381          1382          1383          1384          1385          1386          1387 
 0.3337166147  0.5337070677  0.7201858995  0.8732241467  0.9834764407  1.0225370543  0.9846114495  0.8648501758  0.7183957933 
         1388          1389          1390          1391          1392          1393          1394          1395          1396 
 0.5595677402  0.4103812107  0.2916172165  0.2310356753  0.1306826164  0.0906942656  0.0734907721  0.0786585969  0.0714458981 
         1397          1398          1399          1400          1401          1402          1403          1404          1405 
 0.0857900550  0.0887260376  0.1384793056  0.1682445844  0.2005474506  0.2392320366  0.3182838712  0.3345088723  0.3779200705 
         1406          1407          1408          1409          1410          1411          1412          1413          1414 
 0.3786713037  0.4202309809  0.4547126444  0.4960511329  0.5222282188  0.5988541003  0.6116063370  0.6136941778  0.6370236907 
         1415          1416          1417          1418          1419          1420          1421          1422          1423 
 0.6319059045  0.6213505659  0.6190571626  0.5923550845  0.5849542257  0.5792166108  0.5570410100  0.5531153599  0.5310454763 
         1424          1425          1426          1427          1428          1429          1430          1431          1432 
 0.5178849081  0.5112635448  0.4823382313  0.4669021262  0.4465066370  0.4410628227  0.4238275770  0.3990377062  0.3834977819 
         1433          1434          1435          1436          1437          1438          1439          1440          1441 
 0.3515667199  0.3381638684  0.3249125832  0.3232629574  0.2858587629  0.2922728425  0.2549751136  0.2157699902  0.1929361942 
         1442          1443          1444          1445          1446          1447          1448          1449          1450 
 0.1825297726  0.1445154801  0.1540614297  0.1025002302  0.0726636795  0.0662075940  0.0642854899  0.1038151621  0.1185784941 
         1451          1452          1453          1454          1455          1456          1457          1458          1459 
 0.1757423105  0.2027172561  0.2702972558  0.3276114700  0.4223734657  0.4820651412  0.5085756991  0.5030845228  0.5477737639 
         1460          1461          1462          1463          1464          1465          1466          1467          1468 
 0.5538158156  0.5390047459  0.5137279048  0.4590636741  0.4862796375  0.4215293961  0.3692053829  0.3106787656  0.2336189887 
         1469          1470          1471          1472          1473          1474          1475          1476          1477 
 0.1247623797  0.0904308456  0.0102717949  0.0000000000 -0.0045850625 -0.1066950254 -0.1963826229 -0.2331230328 -0.2526693614 
         1478          1479          1480          1481          1482          1483          1484          1485          1486 
-0.2754508682 -0.3825548586 -0.4640960569 -0.4670191577 -0.5896649594 -0.6325803441 -0.6694512463 -0.8038692544 -0.8608068913 
         1487          1488          1489          1490          1491          1492          1493          1494          1495 
-1.0187170742 -1.0840740713 -1.0636558506 -0.8607833646 -1.0163257239 -1.2943239643 -1.2685752402 -1.3159850933 -1.8375525815 
         1496          1497          1498          1499          1500          1501          1502          1503          1504 
-1.3677116381 -0.8679510734 -1.5862672583 -0.9444272391 -1.1218761891 -0.9595962367 -1.0473804764 -1.0260127777 -1.0977525283 
         1505          1506          1507          1508          1509          1510          1511          1512          1513 
-0.4193500535 -0.6906726784 -1.7641849455 -0.1271564866  0.0000000000  0.0031057350 -0.1673101329 -0.4689838117 -0.9518404322 
         1514          1515          1516          1517          1518          1519          1520          1521          1522 
 0.0000000000 -7.6910690530  1.7598332134  0.4202517155  0.5548469549  2.5552004548  1.7083742240  1.6040002012  0.8596821851 
         1523          1524          1525          1526          1527          1528          1529          1530          1531 
 1.8718946223  0.6534636570  0.3208728697  2.2258827577  1.3664641225  1.6769558412  1.6110377603  0.2377595195  1.1218281923 
         1532          1533          1534          1535          1536          1537          1538          1539          1540 
 1.8007426133  0.7662496934  0.0000000000  0.4106071942  1.5849590969  1.1527982292 -0.2618267342  1.0477595934 -0.4792902714 
         1541          1542          1543          1544          1545          1546          1547          1548          1549 
 0.4371787453  0.2227269032  0.3830251716 -1.3257639638 -1.1058745608 -0.0711414794 -0.0582154915 -0.0495778299 -0.3844554041 
         1550          1551          1552          1553          1554          1555          1556          1557          1558 
-0.4942754747 -0.4732186321 -0.6518355847 -0.1088294593 -0.4765346258 -0.3498479149 -0.2706217428 -0.2583259102 -0.1971987100 
         1559          1560          1561          1562          1563          1564          1565          1566          1567 
-0.2958381506 -0.1463682710 -0.2358091006 -0.3187742126 -0.2785724752 -0.1599770380 -0.1297281918 -0.1599751518 -0.1704656128 
         1568          1569          1570          1571          1572          1573          1574          1575          1576 
-0.1622542797 -0.1989434094 -0.2111895297 -0.1495659788 -0.1787516928 -0.1645361267 -0.1263198727 -0.0212154207 -0.0788105847 
         1577          1578          1579          1580          1581          1582          1583          1584          1585 
-0.0790124337 -0.0291065128  0.0000000000  0.0000000000  0.0731999362  0.1139426252  0.1978346044  0.2687019828  0.2199576013 
         1586          1587          1588          1589          1590          1591          1592          1593          1594 
 0.3128020853  0.3296211674  0.3860770387  0.3516451092  0.3405691427  0.2964860008  0.2888011089  0.2797129850  0.1955081976 
         1595          1596          1597          1598          1599          1600          1601          1602          1603 
 0.1270981230  0.0415196514  0.0225597522  0.0372135934  0.0148678599  0.0000000000  0.0000000000  0.0000000000  0.0000000000 
         1604          1605          1606          1607          1608          1609          1610          1611          1612 
 0.0006209133  0.0000000000  0.0000000000  0.0000000000  0.0000000000 -0.0079679556 -0.0575716106 -0.0498858122 -0.1346845794 
         1613          1614          1615          1616          1617          1618          1619          1620          1621 
-0.1455810289 -0.1262518771 -0.1349513969 -0.1396381353 -0.1384496867 -0.1277378285 -0.2103146594 -0.2241249972 -0.2586371680 
         1622          1623          1624          1625          1626          1627          1628          1629          1630 
-0.3349159601 -0.3550399466 -0.4075458890 -0.4638451539 -0.5026609061 -0.5414947794 -0.5179712722 -0.5195125964 -0.4524625917 
         1631          1632          1633          1634          1635          1636          1637          1638          1639 
-0.3138210344 -0.1171915625  0.0000000000  0.1873713664  0.4065055095  0.6421392540  0.8789369688  1.1137645214  1.3199276571 
         1640          1641          1642          1643          1644          1645          1646          1647          1648 
 1.4690754245  1.5328019087  1.4794597700  1.3005003853  1.0100259137  0.6552114244  0.3049281564  0.0058748276 -0.0137451674 
         1649          1650          1651          1652          1653          1654          1655          1656          1657 
-0.2616764419 -0.4660842515 -0.6216251330 -0.7474288076 -0.8484381056 -0.9277142764 -1.0061452437 -1.0708986582 -1.1490646788 
         1658          1659          1660          1661          1662          1663          1664          1665          1666 
-1.2213152079 -1.2951703699 -1.4136048478 -1.5203075233 -1.5589512492 -1.6137724508 -1.6800272033 -1.7321032139 -1.7650365987 
         1667          1668          1669          1670          1671          1672          1673          1674          1675 
-1.8074224874 -1.8501003630 -1.8578626082 -1.8393073573 -1.8029685005 -1.7345158973 -1.6292552245 -1.5233250732 -1.4233590636 
         1676          1677          1678          1679          1680          1681          1682          1683          1684 
-1.3640414518 -1.3323055450 -1.3110657481 -1.2908924263 -1.2692028765 -1.2495809557 -1.2417340295 -1.2158128061 -1.2023925016 
         1685          1686          1687          1688          1689          1690          1691          1692          1693 
-1.1597439513 -1.1373677120 -1.0932256146 -1.0506267662 -1.0089963603 -0.9562059799 -0.9245333833 -0.8904338523 -0.8421751712 
         1694          1695          1696          1697          1698          1699          1700          1701          1702 
-0.8224525956 -0.7862999019 -0.7532638065 -0.6927510414 -0.6389194501 -0.5837564578 -0.4903101370 -0.4134550663 -0.2998666651 
         1703          1704          1705          1706          1707          1708          1709          1710          1711 
-0.2004652028 -0.0584366172  0.0000000000  0.1204597125  0.2688178271  0.3713637289  0.4450034279  0.5092171824  0.6033182729 
         1712          1713          1714          1715          1716          1717          1718          1719          1720 
 0.7182532943  0.8199430549  0.9158355504  0.9963390098  1.0044086909  1.0428442211  0.9970925475  1.0200360622  0.9968351052 
         1721          1722          1723          1724          1725          1726          1727          1728          1729 
 0.9673860398  0.9704001915  0.9408805516  0.9314137020  0.9343754306  0.9117664394  0.8803921976  0.8327724392  0.8348765434 
         1730          1731          1732          1733          1734          1735          1736          1737          1738 
 0.7532851738  0.6827112193  0.6159726089  0.5209837978  0.4175937209  0.3463147952  0.2379151637  0.1154200096  0.0000000000 
         1739          1740          1741          1742          1743          1744          1745          1746          1747 
 0.0000000000  0.0000000000  0.0000000000 -0.0562966079 -0.0926089434 -0.0385092102 -0.0713216837 -0.1422390353 -0.1841375321 
         1748          1749          1750          1751          1752          1753          1754          1755          1756 
-0.2787825995 -0.0995187041 -0.1828147464  0.0000000000 -0.1595479753  0.0000000000 -0.2807677040 -0.0105471819  0.0000000000 
         1757          1758          1759          1760          1761          1762          1763          1764          1765 
-0.0034615564  0.0000000000 -0.1930353611 -0.0163824510  0.0000000000 -0.0834651441  0.0000000000 -0.2126781652 -0.0053968587 
         1766          1767          1768          1769          1770          1771 
-0.2725555050 -0.8682901366  0.0000000000 -0.4185970146 -0.3243462917 -0.6472410097 

 number of the nonzero coefficients:
[1] 704

 alpha: 0.02
 lambda: 0.01
 lambdaw: 0.003971358
# References 

Friedman J., Hastie T. and Tibshirani R. (2010) Regularization paths for generalized linear 
models via coordinate descent. Journal of Statistical Software, 33(1), 1-22. DOI
[10.1163/ej.9789004178922.i-328.7](https://www.jstatsoft.org/article/download/v033i01/361)

Reinhold, W. C., Sunshine, M., Liu, H., Varma, S., Kohn, K. W., Morris, J., Doroshow, J., &
Pommier, Y. (2012). CellMiner: A web-based suite of genomic and pharmacologic tools to
explore transcript and drug patterns in the NCI-60 cell line set. Cancer Research, 72(14),
3499–3511. DOI
[10.1158/0008-5472.can-12-1370](https://pubmed.ncbi.nlm.nih.gov/22802077/)

A. Alfons (2021). robustHD: An R package for robust regression with high-dimensional data. 
Journal of Open Source Software, 6(67), 3786. DOI
[10.21105/joss.03786](https://joss.theoj.org/papers/10.21105/joss.03786)
